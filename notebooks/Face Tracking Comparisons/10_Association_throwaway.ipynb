{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies`\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import mmcv, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from JAVER.track_tools import FrameHandler, FaceTracker\n",
    "\n",
    "import logging\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    logging.getLogger(name).setLevel(\"CRITICAL\") \n",
    "    \n",
    "from JAVER.logger import create_logger\n",
    "logger = create_logger(level='DEBUG')\n",
    "logger.setLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup face tracker\n",
    "p = Path(\"../../tests/data/Elon Musk/\")\n",
    "ref_paths = list(p.glob(\"inliers/*\"))\n",
    "\n",
    "face_tracker = FaceTracker(\n",
    "    image_size=160, \n",
    "    ref_paths=ref_paths, \n",
    "    batch_size=8, \n",
    "    step_large=150, \n",
    "    step_small=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-19 13:35:34,632] \u001b[32mINFO @ line 374: Rollback to 0\u001b[0m\n",
      "[2020-04-19 13:35:34,634] \u001b[33mWARNING @ line 261: Step size too big!!\u001b[0m\n",
      "[2020-04-19 13:35:39,712] \u001b[32mINFO @ line 398: Rollforward to 410 (Current tail @ 480)\u001b[0m\n",
      "[2020-04-19 13:35:39,715] \u001b[33mWARNING @ line 261: Step size too big!!\u001b[0m\n",
      "[2020-04-19 13:35:39,960] \u001b[32mINFO @ line 374: Rollback to 710\u001b[0m\n",
      "[2020-04-19 13:35:39,961] \u001b[33mWARNING @ line 261: Step size too big!!\u001b[0m\n",
      "[2020-04-19 13:35:44,506] \u001b[32mINFO @ line 402: Finished Tracking\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Single face video\n",
    "video_path = '../../tests/data/Elon Musk/sample_short.mp4'\n",
    "faces_dict_short = face_tracker.detect(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../tests/data/Elon Musk/sample_short.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Video(video_path, width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi face video\n",
    "video_path = '../../tests/data/Elon Musk/multi_person_jre.mp4'\n",
    "faces_dict_long = face_tracker.detect(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../tests/data/Elon Musk/multi_person_jre.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Video(video_path, width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Association "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, face):\n",
    "        # Kalman stuff\n",
    "        x1, y1, x2, y2 = face.bbox\n",
    "        x = np.mean((x1, x2))\n",
    "        y = np.mean((y1, y2))\n",
    "        s = np.linalg.norm((x1 - x2, y1 - y2))\n",
    "\n",
    "        self.dt = dt = 0.1\n",
    "\n",
    "        self.state_x = np.array([x, y, s, 0, 0, 0])\n",
    "        self.state_prev_x = self.state_x\n",
    "\n",
    "        self.state_cov = P = np.diag(np.ones(self.state_x.shape))\n",
    "\n",
    "        self.H = np.asarray([\n",
    "            [1, 0, 0, dt, 0, 0],\n",
    "            [0, 1, 0, 0, dt, 0],\n",
    "            [0, 0, 1, 0, 0, dt],\n",
    "            [0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "        # confidence probably needs tuning\n",
    "        conf = 1\n",
    "        self.R = np.diag(np.ones(len(self.state_x))) * conf\n",
    "\n",
    "        # Deep sort stuff\n",
    "        self.gallery = []\n",
    "        self.none_count = 0\n",
    "        pass\n",
    "\n",
    "    def predict_state(self):\n",
    "        x_now = self.state_x\n",
    "        P_now = self.state_cov\n",
    "        H = self.H\n",
    "\n",
    "        x_pred = H @ x_now\n",
    "        P_pred = H @ P_now @ H.T\n",
    "\n",
    "        return x_pred, P_pred\n",
    "\n",
    "    def update(self, face):\n",
    "        if face is None:\n",
    "            self.none_count += 1\n",
    "        else:\n",
    "            self.update_gallery(face)\n",
    "            self.update_state(face)\n",
    "            self.none_count = 0\n",
    "        return\n",
    "\n",
    "    def update_state(self, face):\n",
    "        z = self.format_measurement(face)\n",
    "        \n",
    "        x_now = self.state_x\n",
    "        P_now = self.state_cov\n",
    "\n",
    "        H = self.H\n",
    "        R = self.R\n",
    "\n",
    "        K = P_now @ H.T @ np.linalg.inv(H @ P_now @ H.T + R)\n",
    "\n",
    "        x_next = x_now + K @ (z - H @ x_now)\n",
    "        P_next = P_now - K @ H @ P_now\n",
    "\n",
    "        self.state_prev_x = self.state_x\n",
    "        self.state_x = x_next\n",
    "        self.state_cov = P_next / np.linalg.norm(P_next)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def format_measurement(self, face):\n",
    "        x1, y1, x2, y2 = face.bbox\n",
    "        _, _, _, x_prev, y_prev, s_prev = self.state_prev_x\n",
    "\n",
    "        x = np.mean((x1, x2))\n",
    "        y = np.mean((y1, y2))\n",
    "        s = np.linalg.norm((x1 - x2, y1 - y2))\n",
    "        xv = x - x_prev\n",
    "        yv = y - y_prev\n",
    "        sv = s - s_prev\n",
    "        z = np.array([x, y, s, xv, yv, sv])\n",
    "        return z\n",
    "\n",
    "\n",
    "    def update_gallery(self, face):\n",
    "        self.gallery.append(face)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JAVER.track_tools import Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _d_1(face, track):\n",
    "    \"\"\"Motion Descriptor\"\"\"\n",
    "    assert type(face) == Face\n",
    "    assert type(track) == Track\n",
    "\n",
    "    z = track.format_measurement(face)  # new measurement vector\n",
    "\n",
    "    y, S = track.predict_state()  # should be next (x,y) predict by kalman\n",
    "    \n",
    "    # normalize each array\n",
    "    z = z / np.linalg.norm(z)\n",
    "    y = y / np.linalg.norm(y)\n",
    "    S = S / np.linalg.norm(S)\n",
    "    \n",
    "    dist = mahalanobis(z, y, S)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def _d_2(face, track):\n",
    "    \"\"\"Appearance Descriptor\"\"\"\n",
    "    assert type(face) == Face\n",
    "    assert type(track) == Track\n",
    "\n",
    "    dist = 1\n",
    "    r_j = face.embedding\n",
    "    for _face in track.gallery:\n",
    "        r_i = _face.embedding\n",
    "        r_j_dist = r_j.T @ r_i\n",
    "        assert 0 <= r_j_dist <= 1\n",
    "        dist = min(dist, r_j_dist)\n",
    "    return 1 - dist\n",
    "\n",
    "\n",
    "def association_cost_matrix(faces, tracks, lam=0.1):\n",
    "    n_faces = len(faces)\n",
    "    n_tracks = len(tracks)\n",
    "    C = np.zeros((n_faces, n_tracks))\n",
    "    for i, j in it.product(range(n_faces), range(n_tracks)):\n",
    "        C[i, j] = lam * _d_1(faces[i], tracks[j]) + (1 - lam) * _d_2(faces[i], tracks[j])\n",
    "    return C\n",
    "\n",
    "\n",
    "def gate_matrix(faces, tracks, _thresh_1=50, _thresh_2=1):\n",
    "    n_faces = len(faces)\n",
    "    n_tracks = len(tracks)\n",
    "    \n",
    "    G = np.zeros((n_faces, n_tracks))\n",
    "    for i, j in it.product(range(n_faces), range(n_tracks)):\n",
    "        G[i, j] = (_d_1(faces[i], tracks[j]) <= _thresh_1) * (_d_2(faces[i], tracks[j]) <= _thresh_2)\n",
    "    G = G.astype(int)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-16 11:28:31,691] \u001b[36mDEBUG @ line 27: Frame: 10. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,693] \u001b[36mDEBUG @ line 27: Frame: 20. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,695] \u001b[36mDEBUG @ line 27: Frame: 30. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,697] \u001b[36mDEBUG @ line 27: Frame: 40. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,699] \u001b[36mDEBUG @ line 27: Frame: 50. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,701] \u001b[36mDEBUG @ line 27: Frame: 60. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,703] \u001b[36mDEBUG @ line 27: Frame: 70. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,705] \u001b[36mDEBUG @ line 27: Frame: 80. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,710] \u001b[36mDEBUG @ line 27: Frame: 90. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,712] \u001b[36mDEBUG @ line 27: Frame: 100. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,714] \u001b[36mDEBUG @ line 27: Frame: 110. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,716] \u001b[36mDEBUG @ line 27: Frame: 120. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,719] \u001b[36mDEBUG @ line 27: Frame: 130. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,721] \u001b[36mDEBUG @ line 27: Frame: 140. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,723] \u001b[36mDEBUG @ line 27: Frame: 150. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,726] \u001b[36mDEBUG @ line 27: Frame: 160. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,728] \u001b[36mDEBUG @ line 27: Frame: 170. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,730] \u001b[36mDEBUG @ line 27: Frame: 180. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,733] \u001b[36mDEBUG @ line 27: Frame: 190. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,735] \u001b[36mDEBUG @ line 27: Frame: 200. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,738] \u001b[36mDEBUG @ line 27: Frame: 210. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,740] \u001b[36mDEBUG @ line 27: Frame: 220. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,743] \u001b[36mDEBUG @ line 27: Frame: 230. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,746] \u001b[36mDEBUG @ line 27: Frame: 240. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,748] \u001b[36mDEBUG @ line 27: Frame: 250. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,751] \u001b[36mDEBUG @ line 27: Frame: 260. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,754] \u001b[36mDEBUG @ line 27: Frame: 270. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,756] \u001b[36mDEBUG @ line 27: Frame: 280. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,759] \u001b[36mDEBUG @ line 27: Frame: 290. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,762] \u001b[36mDEBUG @ line 27: Frame: 300. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,765] \u001b[36mDEBUG @ line 27: Frame: 310. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,768] \u001b[36mDEBUG @ line 27: Frame: 320. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,771] \u001b[36mDEBUG @ line 27: Frame: 330. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,775] \u001b[36mDEBUG @ line 27: Frame: 340. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,779] \u001b[36mDEBUG @ line 27: Frame: 350. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,782] \u001b[36mDEBUG @ line 27: Frame: 360. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,785] \u001b[36mDEBUG @ line 27: Frame: 370. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,788] \u001b[36mDEBUG @ line 27: Frame: 380. Add face 0 to track 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.3 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-16 11:28:31,791] \u001b[36mDEBUG @ line 27: Frame: 710. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,795] \u001b[36mDEBUG @ line 27: Frame: 720. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,798] \u001b[36mDEBUG @ line 27: Frame: 730. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,801] \u001b[36mDEBUG @ line 27: Frame: 740. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,804] \u001b[36mDEBUG @ line 27: Frame: 750. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,807] \u001b[36mDEBUG @ line 27: Frame: 760. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,811] \u001b[36mDEBUG @ line 27: Frame: 770. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,822] \u001b[36mDEBUG @ line 27: Frame: 780. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,825] \u001b[36mDEBUG @ line 27: Frame: 790. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,828] \u001b[36mDEBUG @ line 27: Frame: 800. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,832] \u001b[36mDEBUG @ line 32: Frame: 810. Track 0 was not associated to any face. none_count=1\u001b[0m\n",
      "[2020-04-16 11:28:31,836] \u001b[36mDEBUG @ line 27: Frame: 820. Add face 0 to track 1\u001b[0m\n",
      "[2020-04-16 11:28:31,837] \u001b[36mDEBUG @ line 38: Frame: 820. Track 1 was not associated to any face. none_count=0\u001b[0m\n",
      "[2020-04-16 11:28:31,839] \u001b[36mDEBUG @ line 27: Frame: 830. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,840] \u001b[36mDEBUG @ line 27: Frame: 840. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,842] \u001b[36mDEBUG @ line 27: Frame: 850. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,844] \u001b[36mDEBUG @ line 27: Frame: 860. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,846] \u001b[36mDEBUG @ line 27: Frame: 870. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,848] \u001b[36mDEBUG @ line 27: Frame: 880. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,851] \u001b[36mDEBUG @ line 27: Frame: 890. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,853] \u001b[36mDEBUG @ line 27: Frame: 900. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,855] \u001b[36mDEBUG @ line 27: Frame: 910. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,857] \u001b[36mDEBUG @ line 27: Frame: 920. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,859] \u001b[36mDEBUG @ line 27: Frame: 930. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,862] \u001b[36mDEBUG @ line 27: Frame: 940. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,864] \u001b[36mDEBUG @ line 27: Frame: 950. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,866] \u001b[36mDEBUG @ line 27: Frame: 960. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,869] \u001b[36mDEBUG @ line 27: Frame: 970. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,871] \u001b[36mDEBUG @ line 27: Frame: 980. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,873] \u001b[36mDEBUG @ line 27: Frame: 990. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,876] \u001b[36mDEBUG @ line 27: Frame: 1000. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,878] \u001b[36mDEBUG @ line 27: Frame: 1010. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,881] \u001b[36mDEBUG @ line 27: Frame: 1020. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,883] \u001b[36mDEBUG @ line 27: Frame: 1030. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,886] \u001b[36mDEBUG @ line 27: Frame: 1040. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,888] \u001b[36mDEBUG @ line 27: Frame: 1050. Add face 0 to track 0\u001b[0m\n",
      "[2020-04-16 11:28:31,891] \u001b[36mDEBUG @ line 27: Frame: 1060. Add face 0 to track 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# from JAVER.track_tools import association_cost_matrix, gate_matrix, Track\n",
    "\n",
    "salt = 1 / 10 ** 9\n",
    "pepper = 10 ** 6\n",
    "\n",
    "tracks_alive = []\n",
    "tracks_dead = []\n",
    "\n",
    "lam = 0.1\n",
    "thresh_1, thresh_2 = 0.6, 0.3\n",
    "kill_thresh = 2\n",
    "\n",
    "for _, (frame, faces) in enumerate(faces_dict_short.items()):\n",
    "    if _: # except first case\n",
    "    \n",
    "        C = association_cost_matrix(faces, tracks_alive, lam)\n",
    "        G = gate_matrix(faces, tracks_alive, thresh_1, thresh_2) + salt\n",
    "        gated_cost_matrix = C/G\n",
    "        assert C.shape == G.shape, f'{C.shape}, {G.shape}'\n",
    "        row_idxs, col_idxs = linear_sum_assignment(gated_cost_matrix)\n",
    "\n",
    "        # row_idxs[i] is assigned to col_idxs[j]\n",
    "        # (faces[ri] is assigned to tracks_alive[ci])\n",
    "        for ri, ci in zip(row_idxs, col_idxs):\n",
    "            if gated_cost_matrix[ri, ci] < pepper:\n",
    "                logger.debug(f'Frame: {frame}. Add face {ri} to track {ci}')\n",
    "                tracks_alive[ci].update(faces[ri])\n",
    "                del faces[ri]\n",
    "            else:\n",
    "                tracks_alive[ci].update(None)\n",
    "                logger.debug(f'Frame: {frame}. Track {ci} was not associated to any face. none_count={tracks_alive[ci].none_count}')\n",
    "                \n",
    "        \n",
    "        for i, track in enumerate(tracks_alive):\n",
    "            if i not in col_idxs:\n",
    "                tracks_alive[i].update(None)\n",
    "                logger.debug(f'Frame: {frame}. Track {ci} was not associated to any face. none_count={tracks_alive[ci].none_count}')\n",
    "            \n",
    "            # Terminate track with continous null updates\n",
    "            if track.none_count >= kill_thresh:\n",
    "                tracks_dead.append(track)\n",
    "                del tracks_alive[i]\n",
    "                logger.debug(f'Frame: {frame}. Track {i} killed.')\n",
    "        \n",
    "    # Generate new tracks if required\n",
    "    for face in faces:\n",
    "        new_track = Track(face)\n",
    "        tracks_alive.append(new_track)\n",
    "        \n",
    "for track in tracks_alive:\n",
    "    tracks_dead.append(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0 has 48 face images\n",
      "Track 1 has 25 face images\n"
     ]
    }
   ],
   "source": [
    "for i, track in enumerate(tracks_dead):\n",
    "    cnt = len(track.gallery)\n",
    "    print(f'Track {i} has {cnt} face images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame_handler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-c44638dad56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'frame_handler' is not defined"
     ]
    }
   ],
   "source": [
    "frame_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-410790f3a3b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "arr = np.random.random(size=(300,1000,1000))\n",
    "ts = list(range(50, 200))\n",
    "xs = np.random.randint(0, 900, size=1000)\n",
    "ys = np.random.randint(0, 900, size=1000)\n",
    "\n",
    "for i in range(100):\n",
    "    arr[ts, xs:xs+100, ys:ys+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-233-31ea6a160b7f>:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  arr[[ts, 10, 4]].shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[[ts, 10, 4]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-f6cc6ecc1498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanual_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_fxfy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# LOAD THE CLIP (subclip 6'51 - 7'01 of a chaplin movie)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/emotion/v3.8/lib/python3.8/site-packages/moviepy/video/tools/tracking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreview\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minterpolators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconvert_to_seconds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_clip_fps_by_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/emotion/v3.8/lib/python3.8/site-packages/moviepy/video/io/preview.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from moviepy.editor import *\n",
    "from moviepy.video.tools.tracking import manual_tracking, to_fxfy\n",
    "\n",
    "# LOAD THE CLIP (subclip 6'51 - 7'01 of a chaplin movie)\n",
    "clip = VideoFileClip(\"../../videos/chaplin.mp4\").subclip((6,51.7),(7,01.3))\n",
    "\n",
    "# MANUAL TRACKING OF THE HEAD\n",
    "\n",
    "# the three next lines are for the manual tracking and its saving\n",
    "# to a file, it must be commented once the tracking has been done\n",
    "# (after the first run of the script for instance).\n",
    "# Note that we save the list (ti,xi,yi), not the functions fx and fy\n",
    "# (that we will need) because they have dependencies.\n",
    "\n",
    "#txy, (fx,fy) = manual_tracking(clip, fps=6)\n",
    "#with open(\"../../chaplin_txy.dat\",'w+') as f:\n",
    "#    pickle.dump(txy)\n",
    "\n",
    "\n",
    "\n",
    "# IF THE MANUAL TRACKING HAS BEEN PREVIOUSLY DONE,\n",
    "# LOAD THE TRACKING DATA AND CONVERT IT TO FUNCTIONS x(t),fy(t)\n",
    "\n",
    "with open(\"../../chaplin_txy.dat\",'r') as f:\n",
    "    fx,fy = to_fxfy( pickle.load(f) )\n",
    "\n",
    "\n",
    "# BLUR CHAPLIN'S HEAD IN THE CLIP\n",
    "\n",
    "clip_blurred = clip.fx( vfx.headblur, fx, fy, 25)\n",
    "\n",
    "\n",
    "# Generate the text, put in on a grey background\n",
    "\n",
    "txt = TextClip(\"Hey you ! \\n You're blurry!\", color='grey70',\n",
    "               size = clip.size, bg_color='grey20',\n",
    "               font = \"Century-Schoolbook-Italic\", fontsize=40)\n",
    "               \n",
    "               \n",
    "# Concatenate the Chaplin clip with the text clip, add audio\n",
    "\n",
    "final = concatenate_videoclips([clip_blurred,txt.set_duration(3)]).\\\n",
    "          set_audio(clip.audio)\n",
    "\n",
    "# We write the result to a file. Here we raise the bitrate so that\n",
    "# the final video is not too ugly.\n",
    "\n",
    "final.write_videofile('../../blurredChaplin.avi', bitrate=\"3000k\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('v3.8': venv)",
   "language": "python",
   "name": "python38164bitv38venv18d7852268ab49a4bd6e7f05b2131277"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
